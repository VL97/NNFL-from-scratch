{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive bayes is based on bayes theorem: <br> <center>\n",
    "    <b> p(a|b) = (p(b|a)*p(a))/p(b) </b>\n",
    "</br> </center> <br>\n",
    "In our case a will be target label and b is feature vector. We will assume all feature are mutually independent. <br>In our dataset [Source: https://www.kaggle.com/msjaiclub/2classclassification?select=ex2data1.csv] a person being healthy or not is our target and features are person-go-for-walk and person-eat-healthy-food so both are independent but both contribute in target.\n",
    "\n",
    "using that independent assumption now we can divide probability for every feature by converting <br> <center>\n",
    "    <b>p(a|b) = { (p(b1|a)*p(b2|a)*p(b3|a)....*p(bn|a)) * p(a) } / p(b) </b>\n",
    "    </br> </center>\n",
    "<br>\n",
    "where p(a|b) is conditional probability <br>\n",
    "p(b1|a) is class posterior probability <br>\n",
    "p(a) = prior prob of a <br>\n",
    "p(b) = prior prob of b <br>\n",
    "\n",
    "Now we will select class which is highest so we will apply <br> <center> \n",
    "    <b>argmax a {p(a|b)} <br></b> or <b><br> argmax a { (p(b1|a)*p(b2|a)*p(b3|a)....*p(bn|a)) * p(a) } / p(b) </b> <br> </center> <br>\n",
    "and we will take log of this multiplication because this all term is between 0 to 1 can cause overflow so convert it by taking log.\n",
    "\n",
    "So final formula becomes <b> <br> <center> argmax a {log(p(b1|a)) + .... + log(p(bn|a) + log(p(a)) } </center> <br>  </b>\n",
    "where prior prob p(a) is just frequency <br>\n",
    "and posterior p(bi|a) for all i = 1...n features follows gaussian kernel.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "\n",
    "    def fittodata(self, data, target):\n",
    "        samples, features = data.shape       \n",
    "        #print(samples, features)\n",
    "        \n",
    "        #diffrent element in target array\n",
    "        self._classes = np.unique(target)    \n",
    "        classes = len(self._classes)        \n",
    "        \n",
    "        #  # calculate mean, var, and prior for each class.\n",
    "        self._mean = np.zeros((classes, features), dtype=np.float64)  \n",
    "        self._var = np.zeros((classes, features), dtype=np.float64)\n",
    "        self._priors =  np.zeros(classes, dtype=np.float64)    \n",
    "\n",
    "        #doing for every unique class\n",
    "        for i, c in enumerate(self._classes):  \n",
    "            # we select all data with one class, like all data with target 0\n",
    "            data_c = data[target==c]  \n",
    "            \n",
    "            # we calculate mean and var of each feature column in data_c ie\n",
    "            # the restricted set when a particular class c is selected.\n",
    "            # We want to determine p(b==x|a==c) = p(b0==x0|a==c)*...*p(bn==xn|a==c)\n",
    "            \n",
    "            # column-mean for different features (and particular class c) stored in array  \n",
    "            self._mean[i, :] = data_c.mean(axis=0)  \n",
    "            # variance array for different features (and particular class c) stored in array \n",
    "            self._var[i, :] = data_c.var(axis=0)\n",
    "            \n",
    "            # above 2 help us determine the marginal distributions of each feature\n",
    "            # ie array [p(b0|a==c), p(bn|a==c)] for this particular 2 feature dataset.\n",
    "            \n",
    "            # prior probability of observing class c with class index i ie p(a==c)\n",
    "            self._priors[i] = data_c.shape[0] / float(samples) \n",
    "        \n",
    "        # Side note: below signifies mean for the 2nd-feature for target class index 0: ie self._mean[0][1]        \n",
    "        \n",
    "    def predictdata(self, data):   \n",
    "        #PREDICT FOR WHOLE SAMPLE\n",
    "        target_pred = [self._predictdata(x) for x in data]\n",
    "        return np.array(target_pred)      \n",
    "        \n",
    "    def _predictdata(self, x):  \n",
    "        posteriors = []\n",
    "        # for each datasample x find the probability of x belonging to each class (or class's pdf) in self._classes \n",
    "        # take the max of these which maximizes p(a|b) where a is possible class assigned to x. \n",
    "        #rather than addition we multiply as we work with logs\n",
    "        \n",
    "        for i, c in enumerate(self._classes): \n",
    "            # we assume that x belongs to class c out of possible self._classes with class index i \n",
    "            # then log  (priori-probability of x belonging to class index i) or p(a==c) is:\n",
    "            prior = np.log(self._priors[i])\n",
    "            # since x belongs to class c with class index i, the probability of belonging to class c\n",
    "            # depends on x0 and x1 coming from joint distribution of b0 and b1 calculated as\n",
    "            # gaussian distribution from data particular to class c.\n",
    "            \n",
    "            # _pdfdata returns array as the probabilities of x0 belonging to b0's distribution\n",
    "            # and x1 belonging to b1's distribution ie [p(b0==x0|a==c), p(b1==x1|a==c)]\n",
    "            \n",
    "            # rather than multiplying as we do in joint probabilities we do sum as logs were taken.\n",
    "            posterior = np.sum(np.log(self._pdfdata(i, x)))\n",
    "            \n",
    "            # below forms the log version of eqaution: \n",
    "            # (p(b1|a)p(b2|a)p(b3|a)....*p(bn|a)) * p(a)\n",
    "            posterior = posterior + prior\n",
    "            posteriors.append(posterior)\n",
    "            \n",
    "        # return class c which gives highest log(p(a==c|b)) condititonal probability\n",
    "        return self._classes[np.argmax(posteriors)]\n",
    "            \n",
    "\n",
    "    def _pdfdata(self, class_i, x): \n",
    "        mean = self._mean[class_i]\n",
    "        var = self._var[class_i]\n",
    "        \n",
    "        # we return a distribution of each feature ie b0 and b1 when we priori know that x belongs to class_i.     \n",
    "        num = np.exp(- (x-mean)**2 / (2 * var))\n",
    "        deno = np.sqrt(2 * np.pi * var)\n",
    "        # below is an array consisting of the probability of a feature xi belonging to distribution of bi when\n",
    "        # we know priori that x belongs to class c with class index class_i.\n",
    "        return num / deno   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ex2data1.csv') #we have taken a dataset from kaggle[https://www.kaggle.com/msjaiclub/2classclassification?select=ex2data1.csv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34.623660</td>\n",
       "      <td>78.024693</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30.286711</td>\n",
       "      <td>43.894998</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35.847409</td>\n",
       "      <td>72.902198</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60.182599</td>\n",
       "      <td>86.308552</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79.032736</td>\n",
       "      <td>75.344376</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           x          y  label\n",
       "0  34.623660  78.024693      0\n",
       "1  30.286711  43.894998      0\n",
       "2  35.847409  72.902198      0\n",
       "3  60.182599  86.308552      1\n",
       "4  79.032736  75.344376      1"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head() # dataset head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.drop('label',axis = 1) # in data we take all column except target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.asarray(data)  # converting that to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[34.62365962, 78.02469282],\n",
       "       [30.28671077, 43.89499752],\n",
       "       [35.84740877, 72.90219803],\n",
       "       [60.18259939, 86.3085521 ],\n",
       "       [79.03273605, 75.34437644],\n",
       "       [45.08327748, 56.31637178],\n",
       "       [61.10666454, 96.51142588],\n",
       "       [75.02474557, 46.55401354],\n",
       "       [76.0987867 , 87.42056972],\n",
       "       [84.43281996, 43.53339331],\n",
       "       [95.86155507, 38.22527806],\n",
       "       [75.01365839, 30.60326323],\n",
       "       [82.30705337, 76.4819633 ],\n",
       "       [69.36458876, 97.71869196],\n",
       "       [39.53833914, 76.03681085],\n",
       "       [53.97105215, 89.20735014],\n",
       "       [69.07014406, 52.74046973],\n",
       "       [67.94685548, 46.67857411],\n",
       "       [70.66150955, 92.92713789],\n",
       "       [76.97878373, 47.57596365],\n",
       "       [67.37202755, 42.83843832],\n",
       "       [89.67677575, 65.79936593],\n",
       "       [50.53478829, 48.85581153],\n",
       "       [34.21206098, 44.2095286 ],\n",
       "       [77.92409145, 68.97235999],\n",
       "       [62.27101367, 69.95445795],\n",
       "       [80.19018075, 44.82162893],\n",
       "       [93.1143888 , 38.80067034],\n",
       "       [61.83020602, 50.25610789],\n",
       "       [38.7858038 , 64.99568096],\n",
       "       [61.37928945, 72.80788731],\n",
       "       [85.40451939, 57.05198398],\n",
       "       [52.10797973, 63.12762377],\n",
       "       [52.04540477, 69.43286012],\n",
       "       [40.23689374, 71.16774802],\n",
       "       [54.63510555, 52.21388588],\n",
       "       [33.91550011, 98.86943574],\n",
       "       [64.17698887, 80.90806059],\n",
       "       [74.78925296, 41.57341523],\n",
       "       [34.18364003, 75.23772034],\n",
       "       [83.90239366, 56.30804622],\n",
       "       [51.54772027, 46.85629026],\n",
       "       [94.44336777, 65.56892161],\n",
       "       [82.36875376, 40.61825516],\n",
       "       [51.04775177, 45.82270146],\n",
       "       [62.22267576, 52.06099195],\n",
       "       [77.19303493, 70.4582    ],\n",
       "       [97.77159928, 86.72782233],\n",
       "       [62.0730638 , 96.76882412],\n",
       "       [91.5649745 , 88.69629255],\n",
       "       [79.94481794, 74.16311935],\n",
       "       [99.27252693, 60.999031  ],\n",
       "       [90.54671411, 43.39060181],\n",
       "       [34.52451385, 60.39634246],\n",
       "       [50.28649612, 49.80453881],\n",
       "       [49.58667722, 59.80895099],\n",
       "       [97.64563396, 68.86157272],\n",
       "       [32.57720017, 95.59854761],\n",
       "       [74.24869137, 69.82457123],\n",
       "       [71.79646206, 78.45356225],\n",
       "       [75.39561147, 85.75993667],\n",
       "       [35.28611282, 47.02051395],\n",
       "       [56.2538175 , 39.26147251],\n",
       "       [30.05882245, 49.59297387],\n",
       "       [44.66826172, 66.45008615],\n",
       "       [66.56089447, 41.09209808],\n",
       "       [40.45755098, 97.53518549],\n",
       "       [49.07256322, 51.88321182],\n",
       "       [80.27957401, 92.11606081],\n",
       "       [66.74671857, 60.99139403],\n",
       "       [32.72283304, 43.30717306],\n",
       "       [64.03932042, 78.03168802],\n",
       "       [72.34649423, 96.22759297],\n",
       "       [60.45788574, 73.0949981 ],\n",
       "       [58.84095622, 75.85844831],\n",
       "       [99.8278578 , 72.36925193],\n",
       "       [47.26426911, 88.475865  ],\n",
       "       [50.4581598 , 75.80985953],\n",
       "       [60.45555629, 42.50840944],\n",
       "       [82.22666158, 42.71987854],\n",
       "       [88.91389642, 69.8037889 ],\n",
       "       [94.83450672, 45.6943068 ],\n",
       "       [67.31925747, 66.58935318],\n",
       "       [57.23870632, 59.51428198],\n",
       "       [80.366756  , 90.9601479 ],\n",
       "       [68.46852179, 85.5943071 ],\n",
       "       [42.07545454, 78.844786  ],\n",
       "       [75.47770201, 90.424539  ],\n",
       "       [78.63542435, 96.64742717],\n",
       "       [52.34800399, 60.76950526],\n",
       "       [94.09433113, 77.15910509],\n",
       "       [90.44855097, 87.50879176],\n",
       "       [55.48216114, 35.57070347],\n",
       "       [74.49269242, 84.84513685],\n",
       "       [89.84580671, 45.35828361],\n",
       "       [83.48916274, 48.3802858 ],\n",
       "       [42.26170081, 87.10385094],\n",
       "       [99.31500881, 68.77540947],\n",
       "       [55.34001756, 64.93193801],\n",
       "       [74.775893  , 89.5298129 ]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df['label'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    1\n",
       "4    1\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test, target_train, target_test = train_test_split(data,target,test_size = 0.2 ,random_state = 42)  # splitting in train test split with 80/20 ratio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(target_true, target_pred): \n",
    "    accuracy = np.sum(target_true == target_pred) / len(target_true)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaiveBayesClassifier() \n",
    "nb.fittodata(data_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = nb.predictdata(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes classification accuracy 0.8\n"
     ]
    }
   ],
   "source": [
    "print(\"Naive Bayes classification accuracy\", accuracy(target_test, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
