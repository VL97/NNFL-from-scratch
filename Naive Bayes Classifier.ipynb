{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive bayes is based on bayes theorem: \n",
    "<br> <center>\n",
    "    <b> p(a|b) = (p(b|a)*p(a))/p(b) </b>\n",
    "</br> </center> <br>\n",
    "In our case a will be target label and b is feature vector. We will assume all feature are mutually independent. <br>In our dataset [Source: https://www.kaggle.com/msjaiclub/2classclassification?select=ex2data1.csv] a person being healthy or not is our target and features are person-go-for-walk and person-eat-healthy-food so both are independent but both contribute in target.\n",
    "\n",
    "using that independent assumption now we can divide probability for every feature by converting <br> <center>\n",
    "    <b>p(a|b) = { (p(b1|a)*p(b2|a)*p(b3|a)....*p(bn|a)) * p(a) } / p(b) </b>\n",
    "    </br> </center>\n",
    "<br>\n",
    "where p(a|b) is conditional probability <br>\n",
    "p(b1|a) is class posterior probability <br>\n",
    "p(a) = prior prob of a <br>\n",
    "p(b) = prior prob of b <br>\n",
    "\n",
    "Now we will select class which is highest so we will apply <br> <center> \n",
    "    <b>argmax a {p(a|b)} <br></b> or <b><br> argmax a { (p(b1|a)*p(b2|a)*p(b3|a)....*p(bn|a)) * p(a) } / p(b) </b> <br> </center> <br>\n",
    "and we will take log of this multiplication because this all term is between 0 to 1 can cause overflow so convert it by taking log.\n",
    "\n",
    "So final formula becomes <b> <br> <center> argmax a {log(p(b1|a)) + .... + log(p(bn|a) + log(p(a)) } </center> <br>  </b>\n",
    "where prior prob p(a) is just frequency <br>\n",
    "and posterior p(bi|a) for all i = 1...n features follows gaussian kernel.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "\n",
    "    def fittodata(self, data, target):\n",
    "        samples, features = data.shape       \n",
    "        \n",
    "        # diffrent classes in target array\n",
    "        self._classes = np.unique(target)    \n",
    "        classes = len(self._classes)        \n",
    "        \n",
    "        # calculate mean, var, and prior prob for each class.\n",
    "        self._mean = np.zeros((classes, features), dtype=np.float64)  \n",
    "        self._var = np.zeros((classes, features), dtype=np.float64)\n",
    "        self._priors =  np.zeros(classes, dtype=np.float64)    \n",
    "\n",
    "        # doing for every unique class\n",
    "        for i, c in enumerate(self._classes):  \n",
    "            # we select all data with one class, ex- all data with target class 0\n",
    "            data_c = data[target==c]  \n",
    "            \n",
    "            # we calculate mean and var of each feature column in data_c ie\n",
    "            # the restricted set when a particular class c is selected.\n",
    "            # We want to determine p(b=x|a=c) = p(b0=x0|a=c)*...*p(bn=xn|a=c)\n",
    "            \n",
    "            # column-mean for different features (and restricted to class c) stored in array  \n",
    "            self._mean[i, :] = data_c.mean(axis=0)  \n",
    "            # variance array for different features (and restricted to class c) stored in array \n",
    "            self._var[i, :] = data_c.var(axis=0)\n",
    "            \n",
    "            # above 2 help us determine the marginal distributions of each feature\n",
    "            # ie array [pdf(b0|a=c), pdf(b1|a=c)] for this particular 2 feature dataset.\n",
    "            \n",
    "            # prior probability of observing class c with class index i ie p(a==c)\n",
    "            self._priors[i] = data_c.shape[0] / float(samples) \n",
    "        \n",
    "        # Side note: self._mean[0][1] signifies mean for the 2nd-feature for target class index 0     \n",
    "        \n",
    "    def predictdata(self, data):   \n",
    "        #PREDICT FOR WHOLE SAMPLE\n",
    "        target_pred = [self._predictdata(x) for x in data]\n",
    "        return np.array(target_pred)      \n",
    "        \n",
    "    def _predictdata(self, x):  \n",
    "        posteriors = []\n",
    "        # for each datasample x find seprately the probability of x belonging to each class in self._classes \n",
    "        # take the max of these which maximizes p(a|b) where a is possible class assigned to x. \n",
    "        # rather than addition we multiply as we work with logs\n",
    "        \n",
    "        for i, c in enumerate(self._classes): \n",
    "            # we assume that x belongs to class c out of possible self._classes with class index i \n",
    "            # then log  (priori-probability of x belonging to class index i) or p(a==c) is:\n",
    "            prior = np.log(self._priors[i])\n",
    "            \n",
    "            # since x belongs to class c with class index i, the probability of belonging to class c\n",
    "            # depends on x0 and x1 coming from joint distribution of b0 and b1 calculated as\n",
    "            # gaussian distribution from data restricted to class c as constructed earlier.\n",
    "            \n",
    "            # _pdfdata() returns array as the probabilities of x0 belonging to b0's distribution\n",
    "            # and x1 belonging to b1's distribution ie [p(b0=x0|a=c), p(b1=x1|a=c)] when apriori class\n",
    "            # is known to be c.\n",
    "            \n",
    "            # rather than multiplying as we do in joint probabilities we do sum as logs were taken.\n",
    "            # this gives posterior log probability of x coming from distribution of b when we apriori\n",
    "            # know that class it belongs to is c.\n",
    "            posterior = np.sum(np.log(self._pdfdata(i, x)))\n",
    "            \n",
    "            # below forms the log version of eqaution: \n",
    "            # (posterior)( p(b1=x1|a=c)*p(b2=x2|a=c)p(b3=x3|a=c)....*p(bn=xn|a=c) ) * (prior)p(a=c)\n",
    "            posterior = posterior + prior\n",
    "            posteriors.append(posterior)\n",
    "            \n",
    "        # return that class amongst all classes which gives highest log(p(a=c|b=x)) condititonal probability\n",
    "        return self._classes[np.argmax(posteriors)]\n",
    "            \n",
    "\n",
    "    def _pdfdata(self, class_i, x): \n",
    "        mean = self._mean[class_i]\n",
    "        var = self._var[class_i]\n",
    "        \n",
    "        # we return a probability of each feature of x if it belongs to corresponding gaussian distribution of b\n",
    "        # when we know that x comes from class c with class_i as identifying class index.\n",
    "        num = np.exp(- (x-mean)**2 / (2 * var))\n",
    "        deno = np.sqrt(2 * np.pi * var)\n",
    "        \n",
    "        # below is an array! \n",
    "        return num / deno   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset from kaggle[https://www.kaggle.com/msjaiclub/2classclassification?select=ex2data1.csv]\n",
    "df = pd.read_csv('ex2data1.csv') \n",
    "data = df.drop('label',axis = 1)\n",
    "data = np.asarray(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[34.62365962, 78.02469282],\n",
       "       [30.28671077, 43.89499752],\n",
       "       [35.84740877, 72.90219803],\n",
       "       [60.18259939, 86.3085521 ],\n",
       "       [79.03273605, 75.34437644]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    1\n",
       "4    1\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = df['label'] \n",
    "target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test, target_train, target_test = train_test_split(data,target,test_size = 0.2 ,random_state = 42)  # splitting in train test split with 80/20 ratio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(target_true, target_pred): \n",
    "    accuracy = np.sum(target_true == target_pred) / len(target_true)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaiveBayesClassifier() \n",
    "nb.fittodata(data_train, target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = nb.predictdata(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes classification accuracy 0.8\n"
     ]
    }
   ],
   "source": [
    "print(\"Naive Bayes classification accuracy\", accuracy(target_test, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
